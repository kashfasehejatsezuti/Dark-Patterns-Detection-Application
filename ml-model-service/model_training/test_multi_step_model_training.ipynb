{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1ca5070",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "from joblib import dump, load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07685a72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial DataFrame count: 2643\n"
     ]
    }
   ],
   "source": [
    "# Load the CSV file from the model_training folder\n",
    "df = pd.read_csv(\"../model_training/dataset.csv\")\n",
    "print(f\"Initial DataFrame count: {len(df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "765d95ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of selected types for the second-level models\n",
    "dark_pattern_types = [\"Scarcity\", \"Social Proof\", \"Urgency\", \"Misdirection\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2713e69f",
   "metadata": {},
   "source": [
    "# Step 1: First-level model to distinguish between Dark Patterns and Not Dark Patterns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94f0ed7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a boolean mask\n",
    "dark_pattern_mask = df['Type'].isin(dark_pattern_types)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e673cf",
   "metadata": {},
   "source": [
    "## Assign labels for the first-level model (0 for Not Dark Patterns, 1 for Dark Patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "212b021a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['First_Level_Label'] = dark_pattern_mask.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a985eb5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train DataFrame count: 2114\n",
      "Test DataFrame count: 529\n"
     ]
    }
   ],
   "source": [
    "# Split the dataset into training and testing sets for the first-level model\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Train DataFrame count: {len(train_df)}\")\n",
    "print(f\"Test DataFrame count: {len(test_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ec2b5e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pipelines for the first-level models\n",
    "first_level_algorithms = {\n",
    "    \"Multinomial Naive Bayes\": make_pipeline(TfidfVectorizer(), MultinomialNB()),\n",
    "    \"Support Vector Machines\": make_pipeline(TfidfVectorizer(), SVC(kernel='linear')),\n",
    "    \"Random Forest\": make_pipeline(TfidfVectorizer(), RandomForestClassifier())\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "700ca5fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a folder for trained models if it doesn't exist\n",
    "trained_models_folder = \"trained_models\"\n",
    "os.makedirs(trained_models_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4152b8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_level_models_folder = os.path.join(trained_models_folder, \"first_level_models\")\n",
    "os.makedirs(first_level_models_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "727a9372",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial Naive Bayes First-Level Model Accuracy: 0.85\n",
      "Support Vector Machines First-Level Model Accuracy: 0.92\n",
      "Random Forest First-Level Model Accuracy: 0.93\n"
     ]
    }
   ],
   "source": [
    "for algo_name, model in first_level_algorithms.items():\n",
    "    train_df = train_df.dropna(subset=['Text'])\n",
    "    train_df['Text'] = train_df['Text'].astype(str)\n",
    "    train_df = train_df.reset_index(drop=True)\n",
    "    model.fit(train_df['Text'], train_df['label'])\n",
    "    predictions = model.predict(test_df['Text'])\n",
    "    accuracy = accuracy_score(test_df['label'], predictions)\n",
    "    model_path = os.path.join(first_level_models_folder, f'{algo_name.lower().replace(\" \", \"_\")}_model.joblib')\n",
    "    dump(model, model_path)\n",
    "    print(f\"{algo_name} First-Level Model Accuracy: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d075a1d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "testdf = pd.read_csv(\"../model_training/scraped_data/edfc2345.csv\")\n",
    "testpredict = model.predict(testdf['Text'])\n",
    "print(testpredict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "32bf5d30",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "['text']",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Train and evaluate each first-level model\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m algo_name, model \u001b[38;5;129;01min\u001b[39;00m first_level_algorithms\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m      3\u001b[0m     \n\u001b[0;32m      4\u001b[0m     \u001b[38;5;66;03m# Drop rows with missing values in the 'Text' column\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m     train_df \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropna\u001b[49m\u001b[43m(\u001b[49m\u001b[43msubset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;66;03m# Ensure 'Text' column has string data type\u001b[39;00m\n\u001b[0;32m      8\u001b[0m     train_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m train_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mstr\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\amay rajvaidya\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\frame.py:6418\u001b[0m, in \u001b[0;36mDataFrame.dropna\u001b[1;34m(self, axis, how, thresh, subset, inplace, ignore_index)\u001b[0m\n\u001b[0;32m   6416\u001b[0m     check \u001b[38;5;241m=\u001b[39m indices \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   6417\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m check\u001b[38;5;241m.\u001b[39many():\n\u001b[1;32m-> 6418\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(np\u001b[38;5;241m.\u001b[39marray(subset)[check]\u001b[38;5;241m.\u001b[39mtolist())\n\u001b[0;32m   6419\u001b[0m     agg_obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indices, axis\u001b[38;5;241m=\u001b[39magg_axis)\n\u001b[0;32m   6421\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m thresh \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mno_default:\n",
      "\u001b[1;31mKeyError\u001b[0m: ['text']"
     ]
    }
   ],
   "source": [
    "# Train and evaluate each first-level model\n",
    "for algo_name, model in first_level_algorithms.items():\n",
    "    \n",
    "    # Drop rows with missing values in the 'Text' column\n",
    "    train_df = train_df.dropna(subset=['text'])\n",
    "\n",
    "    # Ensure 'Text' column has string data type\n",
    "    train_df['text'] = train_df['text'].astype(str)\n",
    "\n",
    "    # Reset the index of the DataFrame\n",
    "    train_df = train_df.reset_index(drop=True)\n",
    "\n",
    "    # Fit the first-level model\n",
    "    model.fit(train_df['text'], train_df['First_Level_Label'])\n",
    "\n",
    "    # Evaluate the first-level model on the test set\n",
    "    predictions = model.predict(test_df['text'])\n",
    "    accuracy = accuracy_score(test_df['First_Level_Label'], predictions)\n",
    "\n",
    "    # Save the trained first-level model to a file in the first_level_models folder\n",
    "    model_path = os.path.join(first_level_models_folder, f'{algo_name.lower().replace(\" \", \"_\")}_model.joblib')\n",
    "    dump(model, model_path)\n",
    "\n",
    "    print(f\"{algo_name} First-Level Model Accuracy: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f70e48c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0\n",
      " 0 0 0 0 1 1 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 1 0 0 0 1]\n"
     ]
    }
   ],
   "source": [
    "testdf = pd.read_csv(\"../model_training/scraped_data/65afee6814a9699d70727268.csv\")\n",
    "testpredict = model.predict(testdf['Text'])\n",
    "print(testpredict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ea69d778",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Text to predict: Hi how are you\n",
      "\n",
      "First-Level Predictions:\n",
      "Multinomial Naive Bayes: 1\n",
      "Support Vector Machines: 0\n",
      "Random Forest: 0\n"
     ]
    }
   ],
   "source": [
    "# User input text\n",
    "user_input_text = \"Hi how are you\"\n",
    "print(f\"\\nText to predict: {user_input_text}\")\n",
    "\n",
    "# Load and make predictions with the first-level models\n",
    "first_level_predictions = {}\n",
    "\n",
    "for algo_name, model in first_level_algorithms.items():\n",
    "    # Load the saved model\n",
    "    model_path = os.path.join(first_level_models_folder, f'{algo_name.lower().replace(\" \", \"_\")}_model.joblib')\n",
    "    loaded_model = load(model_path)\n",
    "\n",
    "    # Make predictions\n",
    "    first_level_prediction = loaded_model.predict([user_input_text])\n",
    "    \n",
    "    # first_level_prediction[0]: Extracts the actual prediction value from the array.\n",
    "    first_level_predictions[algo_name] = first_level_prediction[0]\n",
    "\n",
    "# Print first-level predictions\n",
    "print(\"\\nFirst-Level Predictions:\")\n",
    "for algo_name, prediction in first_level_predictions.items():\n",
    "    print(f\"{algo_name}: {prediction}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f6ab14",
   "metadata": {},
   "source": [
    "# Step 2: Train second-level models for each Dark Pattern type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2cf5e33a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Dark Pattern containing dataset: 1295\n"
     ]
    }
   ],
   "source": [
    "# Filter the original DataFrame for Dark Pattern types\n",
    "second_level_df = df[df['Type'].isin(dark_pattern_types)]\n",
    "print(f\"Total Dark Pattern containing dataset: {len(second_level_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b7b7d128",
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_scarcity_df = second_level_df[second_level_df['Type'] == 'Scarcity']\n",
    "fake_social_proof_df = second_level_df[second_level_df['Type'] == 'Social Proof']\n",
    "fake_urgency_df = second_level_df[second_level_df['Type'] == 'Urgency']\n",
    "#misdirection_df = second_level_df[second_level_df['Pattern Category'] == 'Misdirection']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5b1006d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fake Scarcity DataFrame Length: 449\n",
      "Fake Social Proof DataFrame Length: 336\n",
      "Fake Urgency DataFrame Length: 251\n"
     ]
    }
   ],
   "source": [
    "# Print the length of each DataFrame\n",
    "print(f\"Fake Scarcity DataFrame Length: {len(fake_scarcity_df)}\")\n",
    "print(f\"Fake Social Proof DataFrame Length: {len(fake_social_proof_df)}\")\n",
    "print(f\"Fake Urgency DataFrame Length: {len(fake_urgency_df)}\")\n",
    "#print(f\"Misdirection DataFrame Length: {len(misdirection_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "425f70fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a folder for trained second-level models if it doesn't exist\n",
    "second_level_models_folder = os.path.join(trained_models_folder, \"second_level_models\")\n",
    "os.makedirs(second_level_models_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "68706472",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Second-Level Models for Scarcity:\n",
      "Positive Dataset Length: 449\n",
      "Negative Dataset Length: 449\n",
      "Positive Train Dataset Length: 359\n",
      "Negative Train Dataset Length: 359\n",
      "Positive Test Dataset Length: 90\n",
      "Negative Test Dataset Length: 90\n",
      "Scarcity - Multinomial Naive Bayes Second-Level Model Accuracy (Positive): 1.00\n",
      "Scarcity - Multinomial Naive Bayes Second-Level Model Accuracy (Negative): 0.94\n",
      "Scarcity - Support Vector Machines Second-Level Model Accuracy (Positive): 1.00\n",
      "Scarcity - Support Vector Machines Second-Level Model Accuracy (Negative): 0.99\n",
      "Scarcity - Random Forest Second-Level Model Accuracy (Positive): 1.00\n",
      "Scarcity - Random Forest Second-Level Model Accuracy (Negative): 1.00\n",
      "\n",
      "Training Second-Level Models for Social Proof:\n",
      "Positive Dataset Length: 336\n",
      "Negative Dataset Length: 336\n",
      "Positive Train Dataset Length: 268\n",
      "Negative Train Dataset Length: 268\n",
      "Positive Test Dataset Length: 68\n",
      "Negative Test Dataset Length: 68\n",
      "Social Proof - Multinomial Naive Bayes Second-Level Model Accuracy (Positive): 0.99\n",
      "Social Proof - Multinomial Naive Bayes Second-Level Model Accuracy (Negative): 0.99\n",
      "Social Proof - Support Vector Machines Second-Level Model Accuracy (Positive): 1.00\n",
      "Social Proof - Support Vector Machines Second-Level Model Accuracy (Negative): 1.00\n",
      "Social Proof - Random Forest Second-Level Model Accuracy (Positive): 1.00\n",
      "Social Proof - Random Forest Second-Level Model Accuracy (Negative): 1.00\n",
      "\n",
      "Training Second-Level Models for Urgency:\n",
      "Positive Dataset Length: 251\n",
      "Negative Dataset Length: 251\n",
      "Positive Train Dataset Length: 200\n",
      "Negative Train Dataset Length: 200\n",
      "Positive Test Dataset Length: 51\n",
      "Negative Test Dataset Length: 51\n",
      "Urgency - Multinomial Naive Bayes Second-Level Model Accuracy (Positive): 0.98\n",
      "Urgency - Multinomial Naive Bayes Second-Level Model Accuracy (Negative): 1.00\n",
      "Urgency - Support Vector Machines Second-Level Model Accuracy (Positive): 0.98\n",
      "Urgency - Support Vector Machines Second-Level Model Accuracy (Negative): 1.00\n",
      "Urgency - Random Forest Second-Level Model Accuracy (Positive): 1.00\n",
      "Urgency - Random Forest Second-Level Model Accuracy (Negative): 1.00\n",
      "\n",
      "Training Second-Level Models for Misdirection:\n",
      "Positive Dataset Length: 259\n",
      "Negative Dataset Length: 259\n",
      "Positive Train Dataset Length: 207\n",
      "Negative Train Dataset Length: 207\n",
      "Positive Test Dataset Length: 52\n",
      "Negative Test Dataset Length: 52\n",
      "Misdirection - Multinomial Naive Bayes Second-Level Model Accuracy (Positive): 0.94\n",
      "Misdirection - Multinomial Naive Bayes Second-Level Model Accuracy (Negative): 1.00\n",
      "Misdirection - Support Vector Machines Second-Level Model Accuracy (Positive): 1.00\n",
      "Misdirection - Support Vector Machines Second-Level Model Accuracy (Negative): 1.00\n",
      "Misdirection - Random Forest Second-Level Model Accuracy (Positive): 1.00\n",
      "Misdirection - Random Forest Second-Level Model Accuracy (Negative): 1.00\n"
     ]
    }
   ],
   "source": [
    "# Iterate over each Dark Pattern type\n",
    "for dark_pattern_type in dark_pattern_types:\n",
    "    print(f\"\\nTraining Second-Level Models for {dark_pattern_type}:\")\n",
    "\n",
    "    # Filter the dataset for the specific Dark Pattern type\n",
    "    positive_df = second_level_df[second_level_df['Type'] == dark_pattern_type]\n",
    "\n",
    "    # Create a combined negative dataset\n",
    "    negative_df = second_level_df[second_level_df['Type'] != dark_pattern_type]\n",
    "\n",
    "    # Ensure that negative_df contains instances with First_Level_Label as 0\n",
    "    negative_df.loc[:, 'First_Level_Label'] = 0\n",
    "\n",
    "    # Ensure that the negative dataset has the same number of instances as the positive dataset\n",
    "    negative_df = negative_df.sample(n=len(positive_df), random_state=42)\n",
    "\n",
    "    # Combine positive and negative datasets\n",
    "    train_combined_df = pd.concat([positive_df, negative_df])\n",
    "\n",
    "    # Shuffle the combined training dataset\n",
    "    train_combined_df = train_combined_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "    # Display the lengths of positive and negative datasets for verification\n",
    "    print(f\"Positive Dataset Length: {len(positive_df)}\")\n",
    "    print(f\"Negative Dataset Length: {len(negative_df)}\")\n",
    "\n",
    "    # Split the datasets into training and testing sets for the second-level model\n",
    "    train_pos_df, test_pos_df = train_test_split(positive_df, test_size=0.2, random_state=42)\n",
    "    train_neg_df, test_neg_df = train_test_split(negative_df, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Display the lengths of positive and negative datasets for verification\n",
    "    print(f\"Positive Train Dataset Length: {len(train_pos_df)}\")\n",
    "    print(f\"Negative Train Dataset Length: {len(train_neg_df)}\")\n",
    "    print(f\"Positive Test Dataset Length: {len(test_pos_df)}\")\n",
    "    print(f\"Negative Test Dataset Length: {len(test_neg_df)}\")\n",
    "\n",
    "    # Create pipelines for the second-level models\n",
    "    second_level_algorithms = {\n",
    "        \"Multinomial Naive Bayes\": make_pipeline(TfidfVectorizer(), MultinomialNB()),\n",
    "        \"Support Vector Machines\": make_pipeline(TfidfVectorizer(), SVC(kernel='linear')),\n",
    "        \"Random Forest\": make_pipeline(TfidfVectorizer(), RandomForestClassifier())\n",
    "    }\n",
    "\n",
    "    # Train and evaluate each second-level model\n",
    "    for algo_name, model in second_level_algorithms.items():\n",
    "        # Drop rows with missing values in the 'Text' column\n",
    "        train_combined_df = train_combined_df.dropna(subset=['Text'])\n",
    "\n",
    "        # Ensure 'Text' column has string data type\n",
    "        train_combined_df['Text'] = train_combined_df['Text'].astype(str)\n",
    "\n",
    "        # Reset the index of the DataFrame\n",
    "        train_combined_df = train_combined_df.reset_index(drop=True)\n",
    "\n",
    "        # Fit the second-level model\n",
    "        model.fit(train_combined_df['Text'], train_combined_df['First_Level_Label'])\n",
    "\n",
    "        # Evaluate the second-level model on the test set (positive and negative combined)\n",
    "        predictions_pos = model.predict(test_pos_df['Text'])\n",
    "        predictions_neg = model.predict(test_neg_df['Text'])\n",
    "\n",
    "        # Calculate accuracy separately for positive and negative datasets\n",
    "        accuracy_pos = accuracy_score(test_pos_df['First_Level_Label'], predictions_pos)\n",
    "        accuracy_neg = accuracy_score(test_neg_df['First_Level_Label'], predictions_neg)\n",
    "\n",
    "        # Save the trained second-level model to a file in the second_level_models folder\n",
    "        model_path = os.path.join(second_level_models_folder, f'{dark_pattern_type.lower().replace(\" \", \"_\")}_{algo_name.lower().replace(\" \", \"_\")}_model.joblib')\n",
    "        dump(model, model_path)\n",
    "\n",
    "        print(f\"{dark_pattern_type} - {algo_name} Second-Level Model Accuracy (Positive): {accuracy_pos:.2f}\")\n",
    "        print(f\"{dark_pattern_type} - {algo_name} Second-Level Model Accuracy (Negative): {accuracy_neg:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d5ef174d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Input Text: Hi how are you\n",
      "\n",
      "First-Level Predictions:\n",
      "Multinomial Naive Bayes: Dark Pattern\n",
      "Support Vector Machines: Not Dark Pattern\n",
      "Random Forest: Not Dark Pattern\n",
      "\n",
      "Second-Level Predictions:\n",
      "\n",
      "Testing for Scarcity:\n",
      "Multinomial Naive Bayes: No\n",
      "Support Vector Machines: No\n",
      "Random Forest: No\n",
      "\n",
      "Testing for Social Proof:\n",
      "Multinomial Naive Bayes: Yes\n",
      "Support Vector Machines: Yes\n",
      "Random Forest: Yes\n",
      "\n",
      "Testing for Urgency:\n",
      "Multinomial Naive Bayes: No\n",
      "Support Vector Machines: Yes\n",
      "Random Forest: No\n"
     ]
    }
   ],
   "source": [
    "def predict_dark_pattern(input_text):\n",
    "    print(f\"\\nInput Text: {input_text}\")\n",
    "\n",
    "    # Load and make predictions with the first-level models\n",
    "    first_level_predictions = {}\n",
    "\n",
    "    print(\"\\nFirst-Level Predictions:\")\n",
    "    for algo_name, model in first_level_algorithms.items():\n",
    "        # Load the saved model\n",
    "        model_path = os.path.join(first_level_models_folder, f'{algo_name.lower().replace(\" \", \"_\")}_model.joblib')\n",
    "        loaded_model = load(model_path)\n",
    "\n",
    "        # Make predictions\n",
    "        first_level_prediction = loaded_model.predict([input_text])\n",
    "\n",
    "        # first_level_prediction[0]: Extracts the actual prediction value from the array.\n",
    "        first_level_predictions[algo_name] = first_level_prediction[0]\n",
    "\n",
    "        print(f\"{algo_name}: {'Dark Pattern' if first_level_prediction[0] == 1 else 'Not Dark Pattern'}\")\n",
    "\n",
    "    # Check if it is identified as a Dark Pattern\n",
    "    if any(value == 1 for value in first_level_predictions.values()):\n",
    "        # Load and make predictions with the second-level models\n",
    "        print(\"\\nSecond-Level Predictions:\")\n",
    "        for dark_pattern_type in dark_pattern_types:\n",
    "            print(f\"\\nTesting for {dark_pattern_type}:\")\n",
    "\n",
    "            # Load the saved second-level model\n",
    "            for algo_name in second_level_algorithms.keys():\n",
    "                model_path = os.path.join(second_level_models_folder, f'{dark_pattern_type.lower().replace(\" \", \"_\")}_{algo_name.lower().replace(\" \", \"_\")}_model.joblib')\n",
    "                loaded_model = load(model_path)\n",
    "\n",
    "                # Make predictions\n",
    "                second_level_prediction = loaded_model.predict([input_text])\n",
    "\n",
    "                # Map numerical labels to Dark Pattern names\n",
    "                dark_pattern_names = {\n",
    "                    0: f\"No\",\n",
    "                    1: f\"Yes\"\n",
    "                }\n",
    "\n",
    "                # Extract the actual prediction value from the array\n",
    "                second_level_prediction_name = dark_pattern_names.get(second_level_prediction[0], \"Unknown\")\n",
    "\n",
    "                print(f\"{algo_name}: {second_level_prediction_name}\")\n",
    "\n",
    "# Example usage\n",
    "user_input_text = \"Hi how are you\"\n",
    "predict_dark_pattern(user_input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2797b3fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
